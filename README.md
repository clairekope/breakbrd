# README

## Pickle Files

Every `.pkl` file contains a dictionary of dictionaries. For the first level, the keys are subhalo IDs. For the second level, the keys are described in the **File Contents** section.

### Reading a File

As an example, let's read `cut2_M_r_parent.pkl` into **Python 3**. Because these files were created using Python 3, they must be read using Python 3.

```python3
import pickle

with open('cut2_M_r_parent.pkl', 'rb') as f:
  cut_M_r_parent = pickle.load(f)
```

### Cut Definitions

Filename | Cut
------------|----
`cut1_particle_info.pkl`    | 1e10 Msun &lt; Mstar &lt; 1e12 Msun & half mass radius &gt; 2 kpc
`cut2_M_r_parent.pkl`       | M_r < -19
`cut3_g-r.pkl`              | g-r > 0.655


### File Contents
#### `cut1_particle_info.pkl`
All of these are Astropy quantities, and have units attached. **PLEASE UPDATE**

- `total_SFR`: Instantaneous SFR for whole galaxy in Msun/yr
- `total_gas`: Total gas mass in Msun
- `total_sfe`: Instantaneous star formation efficiency for whole galaxy in 1/yr
- `inner_SFR`: Instantaneous SFR for r < 2 kpc in Msun/yr
- `inner_gas`: Gas mass for r < 2 kpc in Msun
- `inner_sfe`: Instantaneous star formation efficiency for r < 2 kpc in 1/yr
- `mid_sfe`: As above, for 2 kpc < r < 1 half mass radius
- `far_sfe`: As above, for 1-2 half mass radii
- `outer_sfe`: As above, for r > 2 half mass radii

#### Cuts w/ n > 1
- `M_r`:  Array of M_r for all camera views
- `view`: Camera view with brightest M_r
- `half_mass_rad`: Stellar half mass radius in kpc
- `stellar_mass`: Stellar mass in Msun
- `g-r`: If cut > 2, g-r in the "disk" (Astropy quantity)
- `inner_sSFR_50Myr`/`100Myr`/`1Gyr`: If cut > 3, the inner (r < 2 kpc) time-averaged sSFR. Present in `cut4_radii.pkl` for subhalos that overlap with `cut4_ssfr.pkl`, for some reason?


## Python Scripts
- **download_cutouts** and **download_fits** are for bulk downloading particle cutouts and mock FITs files, respectively. Will exit if using local snapshot data (`--local` flag).
- **gas_analysis** produces the `_gas_info.pkl` files by analysing gas particle cutouts. Whether this information is generted for the parent sample or the g-r sample is controlled by a boolean at the top of the file.
- **get_d4000** post-processes all FSPS spectra of the inner 2 kpc to calculate the D4000 measure (uses Tjitske's function) and saves them in the appropriate `d4000` CSV file (depending on inclusion of dust and instantaneous SFR).
- **illustris_cuts** performs the photometric cuts, generating `cut2_M_r_parent`, and `cut3_g-r.pkl` files.
- **stellar_spectra** generates the mock spectra with FSPS, and will either include or disclude dust or the instantaneous SFR.
- **utilities** contains helper functions for downloading Illustris API data, splitting work among MPI tasks, and dealing with Illustris domain periodicity.

### Obsolete
- **selection_investigation** makes a bunch of plots for initial investigation of cut populations.
- **stellar_mass_growth** determines whether a galaxy is radially inverted or has high sSFR in the center. It creates the `cut4` files and ``cut3_g-r_ssfr.pkl`. The latter contains only the sSFR averaged over the last 1 Gyr for the g-r sample.

### Script Arguments
All scripts use the same set of command line arguments:

+ `z`: redshift; currently either 0.0 or 0.5
+ `-p`,`--parent`: run the analysis for the parent sample. Replaces a boolean set at the top of some of the scripts.
+ `--no-inst`: override default and don't include instantaneous SFR. Replaces a boolean set at the top of some of the scripts.
+ `--no-dust`: override default don't include dust in the spectra. Replaces a boolean set at the top of some of the scripts.
+ `--tng`: use TNG instead of the original Illustris.
+ `-l`,`--local [DIR]`: use a local copy of the full snapshot. The default is the location on Rusty, and will be set accordingly for the `--tng` flag.
+ `-g`,`--gen-mocks`: use FSPS spectra to determine mock magnitude instead of FITS generated by Illustris team.

### Pipeline

To get the information necessary to generate `cut_final`, the scripts should be run as laid out in `python.slurm`. All scripts should be run with the same command line arguments.

## Distributing Work Using MPI and `scatter_work`
```python3
import pickle
from mpi4py import MPI
from utilities import *

comm = MPI.COMM_WORLD
rank = comm.Get_rank()
size = comm.Get_size()

if rank == 0:
    # Assemble the subhalos IDs you want to operate on as a numpy array
    # This example assumes they live in a pickled dictionary
    with open("dict.pkl", "rb") as f:
        dictionary = pickle.read(f)
    subhalo_ids = np.array([k for k in dictionary.keys()])
    
    # Any secondary data to be broadcasted should also be read in on the
    # root processor; for instance, supplementary gas data
    with open("gas_data.pkl", "rb") as f:
        secondary_data = pickle.load(f)
    secondary_data = np.arange(5)
else:
    # Variable names need to be declared for any data you want to distribute
    subhalo_ids = None
    secondary_data = None

# This helper function from utilities.py pads and scatters the arrays
halo_subset = scatter_work(subhalo_ids, rank, size)

# Because scattered arrays have to be the same size, they are padded with -1
good_ids = np.where(halo_subset > -1)[0]

# Broadcast the secondary data normally
secondary_data = comm.bcast(secondary_data, root=0)

my_storage = {} # every rank needs their own way of story results, to be combined later
for halo in halo_subset[good_ids]:
    # do stuff

# Gather the individual results onto one process, stitch together, and save
result_lst = comm.gather(my_storage, root=0)
if rank==0:
    storage = {}
    for dic in result_lst:
        for k, v in dic.items():
            storage[k] = v
    with open("these_results.pkl", "wb") as f:
        pickle.dump(storage, f)
        
# If you want to broadcast the compiled data back out to all processes, add this:
else:
    storage = None
storage = comm.bcast(storage, root=0)

```
